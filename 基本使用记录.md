# 项目安装与更新

环境：windows 10，1660TI

尽量不改路径，选好文件夹cd到位置然后：

```
git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui
```

下载模型，例如：

```
wget https://huggingface.co/Linaqruf/anything-v3.0/resolve/main/anything-v3-fp32-pruned.safetensors
```

> 解说：
> 
> wget是一个用于从Web服务器`下载文件`的命令行工具。它支持HTTP、HTTPS、FTP等协议，可以下载单个文件或整个目录，并支持断点续传和限速等功能。
> 
> git是一个分布式版本控制系统，它可以追踪文件的历史版本，并允许多人协同开发同一个项目。git可以在本地创建仓库，并通过与远程仓库交互来`同步代码`，支持分支管理、合并等高级功能。

这个模型常用于训练lora。可能有网络错误，多试几次。

下载扩展，cd到extension文件夹：

```
git clone https://github.com/kohya-ss/sd-webui-additional-networks
```

可以在webui界面里的extension里用url下载或者available列表寻找下载，但这个我经常下载失败，就用git，再不行，就直接zip下载解压到extension里。

更新失败的时候使用git gui，打开对应文件夹然后启动git，打开repository，选择webui文件夹或要更新的extensioin对应文件夹，然后repository-git bash输入：

```
$ git pull
```

可以把 `git pull` 加入到webui-user.bat的call前一行，每次启动检查更新。

后续实践中发现最新版本有一些bug，甚至无法生成图片。所以不建议太快更新。万一有问题可以备份webui-user.bat后，回退之前版本。
```
git reset <SHA>
```
若有不重要的修改需要忽略，使用 `git checkout .` 后再进行操作。


# 启动设置

编辑webui-user.bat：重点是max_split_size_mb，好像也不是越小越好，试验过32，放大到高度1000+时就会崩，估计是我1660ti的极限了吧。

```
set COMMANDLINE_ARGS = --precision full --no-half --medvram
set PYTORCH_CUDA_ALLOC_CONF = garbage_collection_threshold:0.6, max_split_size_mb:64
```



# 一些基础设置

- setting-stable diffusion: 
  >clip skip:2

- ENSD: Eta noise seed delta: setting->sampler parameters

  <div align=center><img src="media\ENSD.jpg" height="100px" /></div>

- img2img的upscaling选项界面在setting里：

  <div align=center><img src="media\upscaling.jpg" height="100px" /></div>

  但可以保留空格，因为txt2img里的upscale可调，img2img的script里选择upscale也可以再选择。
# 跑图流程

+ txt2img：选择lantent跑出想要的效果，保存种子。

  ```
  prompt: <lora:xxx:0.9>, <lora:xxx:0.8>,(((1girl))),
  
  Negative prompt: easynegative,bad face, lower quality
  
  Steps: 20, 
  Sampler: DPM++ 2M Karras, 
  CFG scale: 7, 
  Seed: XXXXX （-1为随机的意思）
  Face restoration: CodeFormer, 
  Size: 512x768, 
  Model hash: XXXX, 
  Model: XXXX, 
  Clip skip: 2,
  ```
+ 一些常用调整：
  - 颜色：(flat color:1.2), (colorful:1.4), 
  - 质感：(2D:1.1)
  - 视角：(from above:1.3), (from below:1.3),(from side:1.3),(from behind:1.3)
  - 概念：cyberpunk, Renaissance, gothic, mechanical

+ 调整参数到完美
+ txt2img 选择lantent或者SwinIRx4，放大绘画得到成品。（一般都容易炸，最多1.1）(1660TI)

  目前加两个lora，两个controlnet，512x768，先不放大，还能挺住。得到成品图后，在img2img用SwinIRx4放大。
  
  - [b站](https://www.bilibili.com/video/BV12s4y1o7VQ) img2img , 分辨率使用放大前的图的分辨长宽+64(长宽3:4的话大概试试48:64)（没搞懂原理）, 选择 DPM adaptive, denoising:0.2, additional networks:SD upscale, ESRGN系列中的某个。

# 图片文件

生成的PNG格式图片已经含有所有生成参数，可以拖入ui界面自动出现在prompt中（拖到UI显示的参数才最全）。也可以拖入notpad之类文本编辑器中，参数会显示在最开头。所以注意，当手动修改图片时可能会破坏参数信息，注意保存副本。


# 修复脸部

使用inpaint: inpaint masked,fill,whole picture, (),0.5左右
Denoising strength。目前尝试多次修复不会更好，效果就是第一次后的。

# 模型

`stable-diffusion-webui\models` 里的其实都是模型，用在不同的地方。（有时候也会放在插件里的对应模型文件夹里，两种都能检测到）

资源地址：[civitai](https://www.civitai.com)

+ 基础模型：  
  
  `stable-diffusion-webui\models\Stable-diffusion` 中，即UI界面左上角选择的模型，可以是 `safetensors` 格式也可以是 `ckpt` 。

  好用的有：

  ```
  anything-v4.5-pruned
  AbyssOrangeMix
  Counterfeit
  chilloutmix
  Cetus-Mix
  ```

+ LoRA模型：

  多为 `safetensors` 格式。初级阶段我使用插件加载LoRA，模型要放在扩展的模型文件夹里。之后我用 `<lora:xxxx.safetensors:0.9>` 如此格式调用，无需插件，模型放在SD的models文件夹里。

  多人分区使用LoRA见插件介绍。

+ VAE

  目前只知道和细节、颜色有关，如果其他参数和发帖人一样，但是重现不够漂亮可能是这个原因。可以在网上找一些VAE：
  ```
  orangemix.vae.pt
  pastel-waifu-diffusion.vae.pt
  ```

+ controlnet模型：

  较为复杂，见controlnet部分。


# 问题汇总

- [Couldn't launch python, exit code 9009](https://github.com/AUTOMATIC1111/stable-diffusion-webui/issues/1423)

- [No module 'xformers'. Proceeding without it.](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Xformers) 好像是不需要装了

- 试用T2i adapter后controlnet的其他功能也不能用了，每次运行都CUDA爆炸，原因不明。备份模型，重装controlnet插件，解决。 
好像对应这个问题: [T2I Adapter fails when using resolutions that are not a multiple of 64](https://github.com/Mikubill/sd-webui-controlnet/issues/524) 改成64倍数没有解决。自己根据报错提示所要的碎片太小了，我改为`max_split_size_mb:256`，终于成功。但是试了下两个controlnet又报错：
  ```
  torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 6.00 GiB total capacity; 4.42 GiB already allocated; 0 bytes free; 4.46 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
  ```
  看来目前就只加载一个controlnet吧。（后来更新后我又可以最多加载两个controlnet）

# 展示

时隔很久又玩了一下，用c.c.的lora试了不同的风格，用 [Upscayl](https://github.com/upscayl) 放大高清化很方便。模型是 [GhostMix-V2.0-fp16-BakedVAE](https://civitai.com/models/36520/ghostmix) 这里说了BakedVAE意思应该是VAE不需要额外加。(2024-01-17)

![Alt text](txt2img-images/2024-01-17/00016-3333333333_upscayl_4x_realesrgan-x4plus-anime.png)

![Alt text](txt2img-images/2024-01-17/00020-3333333333_upscayl_4x_realesrgan-x4plus-anime.png)

![Alt text](txt2img-images/2024-01-17/00025-3333333333_upscayl_4x_realesrgan-x4plus-anime.png)

![Alt text](txt2img-images/2024-01-17/00024-3333333333_upscayl_4x_realesrgan-x4plus-anime.png)

![Alt text](txt2img-images/2024-01-17/00029-33333333333_upscayl_4x_realesrgan-x4plus-anime.png)

![Alt text](txt2img-images/2024-01-17/00023-3333333333_upscayl_4x_realesrgan-x4plus-anime.png)